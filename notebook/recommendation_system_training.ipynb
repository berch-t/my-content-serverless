{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66e6002b",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "# Système de Recommandation Hybride My Content\n",
    "\n",
    "Ce notebook implémente un système de recommandation hybride combinant:\n",
    "1. **Content-Based Filtering** : Utilise les embeddings d'articles + similarité cosinus\n",
    "2. **Collaborative Filtering** : Utilise la librairie Surprise avec ratings implicites\n",
    "3. **Système Hybride** : Combine intelligemment les deux approches\n",
    "\n",
    "**Objectif** : Recommander 5 articles pertinents pour chaque utilisateur\n",
    "**Architecture** : Compatible GCP Cloud Functions + Next.js frontend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b10ba3fb",
   "metadata": {
    "title": "[python]"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📚 Imports réussis\n"
     ]
    }
   ],
   "source": [
    "# /// script\n",
    "# dependencies = [\n",
    "#     \"pandas>=2.0.0\",\n",
    "#     \"numpy>=1.24.0\",\n",
    "#     \"scikit-learn>=1.3.0\",\n",
    "#     \"scikit-surprise>=1.1.3\",\n",
    "#     \"matplotlib>=3.7.0\",\n",
    "#     \"seaborn>=0.12.0\",\n",
    "#     \"pickle-mixin>=1.0.2\",\n",
    "#     \"jupytext>=1.15.0\"\n",
    "# ]\n",
    "# ///\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Scikit-learn imports\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Surprise library imports (obligatoire selon consignes)\n",
    "from surprise import Dataset, Reader, SVD, NMF\n",
    "from surprise.model_selection import train_test_split, cross_validate\n",
    "from surprise import accuracy\n",
    "\n",
    "# Visualisation\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "print(\"📚 Imports réussis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7713260",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "## 1. Configuration et Chargement des Données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e357a6fd",
   "metadata": {
    "lines_to_next_cell": 1,
    "title": "[python]"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Tous les fichiers de données trouvés\n",
      "📄 Articles metadata: 364047 articles, 5 colonnes\n",
      "🧮 Embeddings: forme (364047, 250)\n"
     ]
    }
   ],
   "source": [
    "def setup_data_paths() -> dict:\n",
    "    \"\"\"Configure les chemins vers les fichiers de données.\"\"\"\n",
    "    base_path = Path(\"../data/news-portal-user-interactions-by-globocom\")\n",
    "    \n",
    "    paths = {\n",
    "        'articles_metadata': base_path / \"articles_metadata.csv\",\n",
    "        'articles_embeddings': base_path / \"articles_embeddings.pickle\", \n",
    "        'clicks_dir': base_path / \"clicks\"\n",
    "    }\n",
    "    \n",
    "    # Vérification existence\n",
    "    for name, path in paths.items():\n",
    "        if not path.exists():\n",
    "            raise FileNotFoundError(f\"❌ Fichier manquant: {path}\")\n",
    "    \n",
    "    print(f\"✅ Tous les fichiers de données trouvés\")\n",
    "    return paths\n",
    "\n",
    "def load_articles_metadata(path: Path) -> pd.DataFrame:\n",
    "    \"\"\"Charge les métadonnées des articles.\"\"\"\n",
    "    df = pd.read_csv(path)\n",
    "    print(f\"📄 Articles metadata: {df.shape[0]} articles, {df.shape[1]} colonnes\")\n",
    "    return df\n",
    "\n",
    "def load_articles_embeddings(path: Path) -> np.ndarray:\n",
    "    \"\"\"Charge les embeddings précalculés des articles.\"\"\"\n",
    "    with open(path, 'rb') as f:\n",
    "        embeddings = pickle.load(f)\n",
    "    print(f\"🧮 Embeddings: forme {embeddings.shape}\")\n",
    "    return embeddings\n",
    "\n",
    "# Configuration des chemins\n",
    "data_paths = setup_data_paths()\n",
    "articles_df = load_articles_metadata(data_paths['articles_metadata'])\n",
    "embeddings_matrix = load_articles_embeddings(data_paths['articles_embeddings'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51b84593",
   "metadata": {
    "title": "[python]"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Chargé clicks_hour_000.csv: 1883 interactions\n",
      "📊 Chargé clicks_hour_001.csv: 1415 interactions\n",
      "📊 Chargé clicks_hour_002.csv: 910 interactions\n",
      "📊 Chargé clicks_hour_003.csv: 621 interactions\n",
      "📊 Chargé clicks_hour_004.csv: 586 interactions\n",
      "📊 Chargé clicks_hour_005.csv: 796 interactions\n",
      "📊 Chargé clicks_hour_006.csv: 1966 interactions\n",
      "📊 Chargé clicks_hour_007.csv: 3412 interactions\n",
      "📊 Chargé clicks_hour_008.csv: 4846 interactions\n",
      "📊 Chargé clicks_hour_009.csv: 5528 interactions\n",
      "📊 Chargé clicks_hour_010.csv: 5272 interactions\n",
      "📊 Chargé clicks_hour_011.csv: 4752 interactions\n",
      "📊 Chargé clicks_hour_012.csv: 5584 interactions\n",
      "📊 Chargé clicks_hour_013.csv: 5109 interactions\n",
      "📊 Chargé clicks_hour_014.csv: 5588 interactions\n",
      "📊 Chargé clicks_hour_015.csv: 6162 interactions\n",
      "📊 Chargé clicks_hour_016.csv: 5630 interactions\n",
      "📊 Chargé clicks_hour_017.csv: 5675 interactions\n",
      "📊 Chargé clicks_hour_018.csv: 6271 interactions\n",
      "📊 Chargé clicks_hour_019.csv: 9239 interactions\n",
      "📊 Chargé clicks_hour_020.csv: 11249 interactions\n",
      "📊 Chargé clicks_hour_021.csv: 9839 interactions\n",
      "📊 Chargé clicks_hour_022.csv: 9304 interactions\n",
      "📊 Chargé clicks_hour_023.csv: 6702 interactions\n",
      "📊 Chargé clicks_hour_024.csv: 4410 interactions\n",
      "📊 Chargé clicks_hour_025.csv: 2494 interactions\n",
      "📊 Chargé clicks_hour_026.csv: 1504 interactions\n",
      "📊 Chargé clicks_hour_027.csv: 1158 interactions\n",
      "📊 Chargé clicks_hour_028.csv: 1447 interactions\n",
      "📊 Chargé clicks_hour_029.csv: 2203 interactions\n",
      "📊 Chargé clicks_hour_030.csv: 4922 interactions\n",
      "📊 Chargé clicks_hour_031.csv: 8703 interactions\n",
      "📊 Chargé clicks_hour_032.csv: 17266 interactions\n",
      "📊 Chargé clicks_hour_033.csv: 19684 interactions\n",
      "📊 Chargé clicks_hour_034.csv: 16209 interactions\n",
      "📊 Chargé clicks_hour_035.csv: 17172 interactions\n",
      "📊 Chargé clicks_hour_036.csv: 20168 interactions\n",
      "📊 Chargé clicks_hour_037.csv: 21395 interactions\n",
      "📊 Chargé clicks_hour_038.csv: 18816 interactions\n",
      "📊 Chargé clicks_hour_039.csv: 17719 interactions\n",
      "📊 Chargé clicks_hour_040.csv: 19160 interactions\n",
      "📊 Chargé clicks_hour_041.csv: 24020 interactions\n",
      "📊 Chargé clicks_hour_042.csv: 20354 interactions\n",
      "📊 Chargé clicks_hour_043.csv: 21375 interactions\n",
      "📊 Chargé clicks_hour_044.csv: 17344 interactions\n",
      "📊 Chargé clicks_hour_045.csv: 15852 interactions\n",
      "📊 Chargé clicks_hour_046.csv: 14942 interactions\n",
      "📊 Chargé clicks_hour_047.csv: 11090 interactions\n",
      "📊 Chargé clicks_hour_048.csv: 6693 interactions\n",
      "📊 Chargé clicks_hour_049.csv: 3941 interactions\n",
      "📊 Chargé clicks_hour_050.csv: 2136 interactions\n",
      "📊 Chargé clicks_hour_051.csv: 1563 interactions\n",
      "📊 Chargé clicks_hour_052.csv: 1576 interactions\n",
      "📊 Chargé clicks_hour_053.csv: 2767 interactions\n",
      "📊 Chargé clicks_hour_054.csv: 6408 interactions\n",
      "📊 Chargé clicks_hour_055.csv: 12667 interactions\n",
      "📊 Chargé clicks_hour_056.csv: 16089 interactions\n",
      "📊 Chargé clicks_hour_057.csv: 14482 interactions\n",
      "📊 Chargé clicks_hour_058.csv: 13030 interactions\n",
      "📊 Chargé clicks_hour_059.csv: 14990 interactions\n",
      "📊 Chargé clicks_hour_060.csv: 16716 interactions\n",
      "📊 Chargé clicks_hour_061.csv: 19645 interactions\n",
      "📊 Chargé clicks_hour_062.csv: 19464 interactions\n",
      "📊 Chargé clicks_hour_063.csv: 17109 interactions\n",
      "📊 Chargé clicks_hour_064.csv: 15828 interactions\n",
      "📊 Chargé clicks_hour_065.csv: 13276 interactions\n",
      "📊 Chargé clicks_hour_066.csv: 9325 interactions\n",
      "📊 Chargé clicks_hour_067.csv: 7008 interactions\n",
      "📊 Chargé clicks_hour_068.csv: 6930 interactions\n",
      "📊 Chargé clicks_hour_069.csv: 6812 interactions\n",
      "📊 Chargé clicks_hour_070.csv: 7018 interactions\n",
      "📊 Chargé clicks_hour_071.csv: 4643 interactions\n",
      "📊 Chargé clicks_hour_072.csv: 3183 interactions\n",
      "📊 Chargé clicks_hour_073.csv: 1815 interactions\n",
      "📊 Chargé clicks_hour_074.csv: 884 interactions\n",
      "📊 Chargé clicks_hour_075.csv: 681 interactions\n",
      "📊 Chargé clicks_hour_076.csv: 881 interactions\n",
      "📊 Chargé clicks_hour_077.csv: 1641 interactions\n",
      "📊 Chargé clicks_hour_078.csv: 3650 interactions\n",
      "📊 Chargé clicks_hour_079.csv: 8767 interactions\n",
      "📊 Chargé clicks_hour_080.csv: 16791 interactions\n",
      "📊 Chargé clicks_hour_081.csv: 16388 interactions\n",
      "📊 Chargé clicks_hour_082.csv: 14616 interactions\n",
      "📊 Chargé clicks_hour_083.csv: 12917 interactions\n",
      "📊 Chargé clicks_hour_084.csv: 12969 interactions\n",
      "📊 Chargé clicks_hour_085.csv: 18001 interactions\n",
      "📊 Chargé clicks_hour_086.csv: 18575 interactions\n",
      "📊 Chargé clicks_hour_087.csv: 14619 interactions\n",
      "📊 Chargé clicks_hour_088.csv: 12020 interactions\n",
      "📊 Chargé clicks_hour_089.csv: 10462 interactions\n",
      "📊 Chargé clicks_hour_090.csv: 8782 interactions\n",
      "📊 Chargé clicks_hour_091.csv: 8620 interactions\n",
      "📊 Chargé clicks_hour_092.csv: 9437 interactions\n",
      "📊 Chargé clicks_hour_093.csv: 9721 interactions\n",
      "📊 Chargé clicks_hour_094.csv: 10008 interactions\n",
      "📊 Chargé clicks_hour_095.csv: 7742 interactions\n",
      "📊 Chargé clicks_hour_096.csv: 4719 interactions\n",
      "📊 Chargé clicks_hour_097.csv: 2565 interactions\n",
      "📊 Chargé clicks_hour_098.csv: 752 interactions\n",
      "📊 Chargé clicks_hour_099.csv: 2 interactions\n",
      "📊 Chargé clicks_hour_100.csv: 0 interactions\n",
      "📊 Chargé clicks_hour_101.csv: 17 interactions\n",
      "📊 Chargé clicks_hour_102.csv: 125 interactions\n",
      "📊 Chargé clicks_hour_103.csv: 506 interactions\n",
      "📊 Chargé clicks_hour_104.csv: 1759 interactions\n",
      "📊 Chargé clicks_hour_105.csv: 2455 interactions\n",
      "📊 Chargé clicks_hour_106.csv: 7536 interactions\n",
      "📊 Chargé clicks_hour_107.csv: 17669 interactions\n",
      "📊 Chargé clicks_hour_108.csv: 16715 interactions\n",
      "📊 Chargé clicks_hour_109.csv: 17030 interactions\n",
      "📊 Chargé clicks_hour_110.csv: 16595 interactions\n",
      "📊 Chargé clicks_hour_111.csv: 15150 interactions\n",
      "📊 Chargé clicks_hour_112.csv: 16822 interactions\n",
      "📊 Chargé clicks_hour_113.csv: 13373 interactions\n",
      "📊 Chargé clicks_hour_114.csv: 10967 interactions\n",
      "📊 Chargé clicks_hour_115.csv: 9972 interactions\n",
      "📊 Chargé clicks_hour_116.csv: 8889 interactions\n",
      "📊 Chargé clicks_hour_117.csv: 7308 interactions\n",
      "📊 Chargé clicks_hour_118.csv: 6226 interactions\n",
      "📊 Chargé clicks_hour_119.csv: 7799 interactions\n",
      "📊 Chargé clicks_hour_120.csv: 5221 interactions\n",
      "📊 Chargé clicks_hour_121.csv: 2680 interactions\n",
      "📊 Chargé clicks_hour_122.csv: 1364 interactions\n",
      "📊 Chargé clicks_hour_123.csv: 1024 interactions\n",
      "📊 Chargé clicks_hour_124.csv: 954 interactions\n",
      "📊 Chargé clicks_hour_125.csv: 1453 interactions\n",
      "📊 Chargé clicks_hour_126.csv: 4183 interactions\n",
      "📊 Chargé clicks_hour_127.csv: 9617 interactions\n",
      "📊 Chargé clicks_hour_128.csv: 18227 interactions\n",
      "📊 Chargé clicks_hour_129.csv: 15078 interactions\n",
      "📊 Chargé clicks_hour_130.csv: 12611 interactions\n",
      "📊 Chargé clicks_hour_131.csv: 11103 interactions\n",
      "📊 Chargé clicks_hour_132.csv: 11033 interactions\n",
      "📊 Chargé clicks_hour_133.csv: 14194 interactions\n",
      "📊 Chargé clicks_hour_134.csv: 12645 interactions\n",
      "📊 Chargé clicks_hour_135.csv: 10373 interactions\n",
      "📊 Chargé clicks_hour_136.csv: 12487 interactions\n",
      "📊 Chargé clicks_hour_137.csv: 13185 interactions\n",
      "📊 Chargé clicks_hour_138.csv: 9982 interactions\n",
      "📊 Chargé clicks_hour_139.csv: 8397 interactions\n",
      "📊 Chargé clicks_hour_140.csv: 9599 interactions\n",
      "📊 Chargé clicks_hour_141.csv: 8556 interactions\n",
      "📊 Chargé clicks_hour_142.csv: 7920 interactions\n",
      "📊 Chargé clicks_hour_143.csv: 7402 interactions\n",
      "📊 Chargé clicks_hour_144.csv: 4649 interactions\n",
      "📊 Chargé clicks_hour_145.csv: 2890 interactions\n",
      "📊 Chargé clicks_hour_146.csv: 1671 interactions\n",
      "📊 Chargé clicks_hour_147.csv: 1212 interactions\n",
      "📊 Chargé clicks_hour_148.csv: 1078 interactions\n",
      "📊 Chargé clicks_hour_149.csv: 1755 interactions\n",
      "📊 Chargé clicks_hour_150.csv: 3848 interactions\n",
      "📊 Chargé clicks_hour_151.csv: 7369 interactions\n",
      "📊 Chargé clicks_hour_152.csv: 9192 interactions\n",
      "📊 Chargé clicks_hour_153.csv: 9896 interactions\n",
      "📊 Chargé clicks_hour_154.csv: 8485 interactions\n",
      "📊 Chargé clicks_hour_155.csv: 7223 interactions\n",
      "📊 Chargé clicks_hour_156.csv: 7747 interactions\n",
      "📊 Chargé clicks_hour_157.csv: 7990 interactions\n",
      "📊 Chargé clicks_hour_158.csv: 7685 interactions\n",
      "📊 Chargé clicks_hour_159.csv: 7038 interactions\n",
      "📊 Chargé clicks_hour_160.csv: 5645 interactions\n",
      "📊 Chargé clicks_hour_161.csv: 5051 interactions\n",
      "📊 Chargé clicks_hour_162.csv: 5763 interactions\n",
      "📊 Chargé clicks_hour_163.csv: 5490 interactions\n",
      "📊 Chargé clicks_hour_164.csv: 4870 interactions\n",
      "📊 Chargé clicks_hour_165.csv: 4748 interactions\n",
      "📊 Chargé clicks_hour_166.csv: 4778 interactions\n",
      "📊 Chargé clicks_hour_167.csv: 4216 interactions\n",
      "📊 Chargé clicks_hour_168.csv: 3738 interactions\n",
      "📊 Chargé clicks_hour_169.csv: 2422 interactions\n",
      "📊 Chargé clicks_hour_170.csv: 1449 interactions\n",
      "📊 Chargé clicks_hour_171.csv: 943 interactions\n",
      "📊 Chargé clicks_hour_172.csv: 1046 interactions\n",
      "📊 Chargé clicks_hour_173.csv: 1044 interactions\n",
      "📊 Chargé clicks_hour_174.csv: 2589 interactions\n",
      "📊 Chargé clicks_hour_175.csv: 4461 interactions\n",
      "📊 Chargé clicks_hour_176.csv: 6203 interactions\n",
      "📊 Chargé clicks_hour_177.csv: 4721 interactions\n",
      "📊 Chargé clicks_hour_178.csv: 5027 interactions\n",
      "📊 Chargé clicks_hour_179.csv: 4456 interactions\n",
      "📊 Chargé clicks_hour_180.csv: 4087 interactions\n",
      "📊 Chargé clicks_hour_181.csv: 4266 interactions\n",
      "📊 Chargé clicks_hour_182.csv: 5584 interactions\n",
      "📊 Chargé clicks_hour_183.csv: 6331 interactions\n",
      "📊 Chargé clicks_hour_184.csv: 6344 interactions\n",
      "📊 Chargé clicks_hour_185.csv: 7369 interactions\n",
      "📊 Chargé clicks_hour_186.csv: 7807 interactions\n",
      "📊 Chargé clicks_hour_187.csv: 7267 interactions\n",
      "📊 Chargé clicks_hour_188.csv: 6846 interactions\n",
      "📊 Chargé clicks_hour_189.csv: 6549 interactions\n",
      "📊 Chargé clicks_hour_190.csv: 7320 interactions\n",
      "📊 Chargé clicks_hour_191.csv: 5836 interactions\n",
      "📊 Chargé clicks_hour_192.csv: 3596 interactions\n",
      "📊 Chargé clicks_hour_193.csv: 2195 interactions\n",
      "📊 Chargé clicks_hour_194.csv: 1188 interactions\n",
      "📊 Chargé clicks_hour_195.csv: 894 interactions\n",
      "📊 Chargé clicks_hour_196.csv: 881 interactions\n",
      "📊 Chargé clicks_hour_197.csv: 1610 interactions\n",
      "📊 Chargé clicks_hour_198.csv: 4023 interactions\n",
      "📊 Chargé clicks_hour_199.csv: 8490 interactions\n",
      "📊 Chargé clicks_hour_200.csv: 18314 interactions\n",
      "📊 Chargé clicks_hour_201.csv: 19767 interactions\n",
      "📊 Chargé clicks_hour_202.csv: 14667 interactions\n",
      "📊 Chargé clicks_hour_203.csv: 13222 interactions\n",
      "📊 Chargé clicks_hour_204.csv: 12501 interactions\n",
      "📊 Chargé clicks_hour_205.csv: 17360 interactions\n",
      "📊 Chargé clicks_hour_206.csv: 18931 interactions\n",
      "📊 Chargé clicks_hour_207.csv: 16784 interactions\n",
      "📊 Chargé clicks_hour_208.csv: 18607 interactions\n",
      "📊 Chargé clicks_hour_209.csv: 19130 interactions\n",
      "📊 Chargé clicks_hour_210.csv: 14564 interactions\n",
      "📊 Chargé clicks_hour_211.csv: 11415 interactions\n",
      "📊 Chargé clicks_hour_212.csv: 10557 interactions\n",
      "📊 Chargé clicks_hour_213.csv: 9703 interactions\n",
      "📊 Chargé clicks_hour_214.csv: 10402 interactions\n",
      "📊 Chargé clicks_hour_215.csv: 8558 interactions\n",
      "📊 Chargé clicks_hour_216.csv: 5117 interactions\n",
      "📊 Chargé clicks_hour_217.csv: 3105 interactions\n",
      "📊 Chargé clicks_hour_218.csv: 1713 interactions\n",
      "📊 Chargé clicks_hour_219.csv: 1395 interactions\n",
      "📊 Chargé clicks_hour_220.csv: 1334 interactions\n",
      "📊 Chargé clicks_hour_221.csv: 2502 interactions\n",
      "📊 Chargé clicks_hour_222.csv: 8159 interactions\n",
      "📊 Chargé clicks_hour_223.csv: 15805 interactions\n",
      "📊 Chargé clicks_hour_224.csv: 28525 interactions\n",
      "📊 Chargé clicks_hour_225.csv: 21455 interactions\n",
      "📊 Chargé clicks_hour_226.csv: 20527 interactions\n",
      "📊 Chargé clicks_hour_227.csv: 19656 interactions\n",
      "📊 Chargé clicks_hour_228.csv: 17972 interactions\n",
      "📊 Chargé clicks_hour_229.csv: 18349 interactions\n",
      "📊 Chargé clicks_hour_230.csv: 12585 interactions\n",
      "📊 Chargé clicks_hour_231.csv: 15341 interactions\n",
      "📊 Chargé clicks_hour_232.csv: 13356 interactions\n",
      "📊 Chargé clicks_hour_233.csv: 11222 interactions\n",
      "📊 Chargé clicks_hour_234.csv: 13840 interactions\n",
      "📊 Chargé clicks_hour_235.csv: 11376 interactions\n",
      "📊 Chargé clicks_hour_236.csv: 10364 interactions\n",
      "📊 Chargé clicks_hour_237.csv: 8006 interactions\n",
      "📊 Chargé clicks_hour_238.csv: 6913 interactions\n",
      "📊 Chargé clicks_hour_239.csv: 7934 interactions\n",
      "📊 Chargé clicks_hour_240.csv: 4236 interactions\n",
      "📊 Chargé clicks_hour_241.csv: 2181 interactions\n",
      "📊 Chargé clicks_hour_242.csv: 1227 interactions\n",
      "📊 Chargé clicks_hour_243.csv: 1007 interactions\n",
      "📊 Chargé clicks_hour_244.csv: 1100 interactions\n",
      "📊 Chargé clicks_hour_245.csv: 1867 interactions\n",
      "📊 Chargé clicks_hour_246.csv: 5413 interactions\n",
      "📊 Chargé clicks_hour_247.csv: 10629 interactions\n",
      "📊 Chargé clicks_hour_248.csv: 19579 interactions\n",
      "📊 Chargé clicks_hour_249.csv: 15656 interactions\n",
      "📊 Chargé clicks_hour_250.csv: 14306 interactions\n",
      "📊 Chargé clicks_hour_251.csv: 13645 interactions\n",
      "📊 Chargé clicks_hour_252.csv: 14559 interactions\n",
      "📊 Chargé clicks_hour_253.csv: 18041 interactions\n",
      "📊 Chargé clicks_hour_254.csv: 16397 interactions\n",
      "📊 Chargé clicks_hour_255.csv: 17113 interactions\n",
      "📊 Chargé clicks_hour_256.csv: 16564 interactions\n",
      "📊 Chargé clicks_hour_257.csv: 14623 interactions\n",
      "📊 Chargé clicks_hour_258.csv: 10722 interactions\n",
      "📊 Chargé clicks_hour_259.csv: 8543 interactions\n",
      "📊 Chargé clicks_hour_260.csv: 8377 interactions\n",
      "📊 Chargé clicks_hour_261.csv: 7395 interactions\n",
      "📊 Chargé clicks_hour_262.csv: 7099 interactions\n",
      "📊 Chargé clicks_hour_263.csv: 5882 interactions\n",
      "📊 Chargé clicks_hour_264.csv: 5072 interactions\n",
      "📊 Chargé clicks_hour_265.csv: 2754 interactions\n",
      "📊 Chargé clicks_hour_266.csv: 1658 interactions\n",
      "📊 Chargé clicks_hour_267.csv: 1105 interactions\n",
      "📊 Chargé clicks_hour_268.csv: 1169 interactions\n",
      "📊 Chargé clicks_hour_269.csv: 1687 interactions\n",
      "📊 Chargé clicks_hour_270.csv: 3606 interactions\n",
      "📊 Chargé clicks_hour_271.csv: 5773 interactions\n",
      "📊 Chargé clicks_hour_272.csv: 5897 interactions\n",
      "📊 Chargé clicks_hour_273.csv: 6099 interactions\n",
      "📊 Chargé clicks_hour_274.csv: 6016 interactions\n",
      "📊 Chargé clicks_hour_275.csv: 6151 interactions\n",
      "📊 Chargé clicks_hour_276.csv: 5814 interactions\n",
      "📊 Chargé clicks_hour_277.csv: 5729 interactions\n",
      "📊 Chargé clicks_hour_278.csv: 5632 interactions\n",
      "📊 Chargé clicks_hour_279.csv: 5349 interactions\n",
      "📊 Chargé clicks_hour_280.csv: 4924 interactions\n",
      "📊 Chargé clicks_hour_281.csv: 5017 interactions\n",
      "📊 Chargé clicks_hour_282.csv: 7236 interactions\n",
      "📊 Chargé clicks_hour_283.csv: 7232 interactions\n",
      "📊 Chargé clicks_hour_284.csv: 6814 interactions\n",
      "📊 Chargé clicks_hour_285.csv: 6388 interactions\n",
      "📊 Chargé clicks_hour_286.csv: 7945 interactions\n",
      "📊 Chargé clicks_hour_287.csv: 6082 interactions\n",
      "📊 Chargé clicks_hour_288.csv: 4461 interactions\n",
      "📊 Chargé clicks_hour_289.csv: 2283 interactions\n",
      "📊 Chargé clicks_hour_290.csv: 1312 interactions\n",
      "📊 Chargé clicks_hour_291.csv: 849 interactions\n",
      "📊 Chargé clicks_hour_292.csv: 932 interactions\n",
      "📊 Chargé clicks_hour_293.csv: 1495 interactions\n",
      "📊 Chargé clicks_hour_294.csv: 3890 interactions\n",
      "📊 Chargé clicks_hour_295.csv: 8401 interactions\n",
      "📊 Chargé clicks_hour_296.csv: 13659 interactions\n",
      "📊 Chargé clicks_hour_297.csv: 12486 interactions\n",
      "📊 Chargé clicks_hour_298.csv: 11719 interactions\n",
      "📊 Chargé clicks_hour_299.csv: 11675 interactions\n",
      "📊 Chargé clicks_hour_300.csv: 12296 interactions\n",
      "📊 Chargé clicks_hour_301.csv: 13682 interactions\n",
      "📊 Chargé clicks_hour_302.csv: 13510 interactions\n",
      "📊 Chargé clicks_hour_303.csv: 11936 interactions\n",
      "📊 Chargé clicks_hour_304.csv: 10288 interactions\n",
      "📊 Chargé clicks_hour_305.csv: 8197 interactions\n",
      "📊 Chargé clicks_hour_306.csv: 6527 interactions\n",
      "📊 Chargé clicks_hour_307.csv: 5354 interactions\n",
      "📊 Chargé clicks_hour_308.csv: 5776 interactions\n",
      "📊 Chargé clicks_hour_309.csv: 5268 interactions\n",
      "📊 Chargé clicks_hour_310.csv: 4783 interactions\n",
      "📊 Chargé clicks_hour_311.csv: 5549 interactions\n",
      "📊 Chargé clicks_hour_312.csv: 3766 interactions\n",
      "📊 Chargé clicks_hour_313.csv: 2289 interactions\n",
      "📊 Chargé clicks_hour_314.csv: 1336 interactions\n",
      "📊 Chargé clicks_hour_315.csv: 920 interactions\n",
      "📊 Chargé clicks_hour_316.csv: 906 interactions\n",
      "📊 Chargé clicks_hour_317.csv: 1129 interactions\n",
      "📊 Chargé clicks_hour_318.csv: 3218 interactions\n",
      "📊 Chargé clicks_hour_319.csv: 5251 interactions\n",
      "📊 Chargé clicks_hour_320.csv: 6198 interactions\n",
      "📊 Chargé clicks_hour_321.csv: 4822 interactions\n",
      "📊 Chargé clicks_hour_322.csv: 5236 interactions\n",
      "📊 Chargé clicks_hour_323.csv: 5215 interactions\n",
      "📊 Chargé clicks_hour_324.csv: 4794 interactions\n",
      "📊 Chargé clicks_hour_325.csv: 5325 interactions\n",
      "📊 Chargé clicks_hour_326.csv: 4954 interactions\n",
      "📊 Chargé clicks_hour_327.csv: 4963 interactions\n",
      "📊 Chargé clicks_hour_328.csv: 4564 interactions\n",
      "📊 Chargé clicks_hour_329.csv: 4682 interactions\n",
      "📊 Chargé clicks_hour_330.csv: 4139 interactions\n",
      "📊 Chargé clicks_hour_331.csv: 3323 interactions\n",
      "📊 Chargé clicks_hour_332.csv: 3122 interactions\n",
      "📊 Chargé clicks_hour_333.csv: 3780 interactions\n",
      "📊 Chargé clicks_hour_334.csv: 4196 interactions\n",
      "📊 Chargé clicks_hour_335.csv: 3266 interactions\n",
      "📊 Chargé clicks_hour_336.csv: 2253 interactions\n",
      "📊 Chargé clicks_hour_337.csv: 1287 interactions\n",
      "📊 Chargé clicks_hour_338.csv: 756 interactions\n",
      "📊 Chargé clicks_hour_339.csv: 558 interactions\n",
      "📊 Chargé clicks_hour_340.csv: 520 interactions\n",
      "📊 Chargé clicks_hour_341.csv: 951 interactions\n",
      "📊 Chargé clicks_hour_342.csv: 1699 interactions\n",
      "📊 Chargé clicks_hour_343.csv: 3001 interactions\n",
      "📊 Chargé clicks_hour_344.csv: 4095 interactions\n",
      "📊 Chargé clicks_hour_345.csv: 3298 interactions\n",
      "📊 Chargé clicks_hour_346.csv: 3425 interactions\n",
      "📊 Chargé clicks_hour_347.csv: 3218 interactions\n",
      "📊 Chargé clicks_hour_348.csv: 3130 interactions\n",
      "📊 Chargé clicks_hour_349.csv: 2912 interactions\n",
      "📊 Chargé clicks_hour_350.csv: 3852 interactions\n",
      "📊 Chargé clicks_hour_351.csv: 5334 interactions\n",
      "📊 Chargé clicks_hour_352.csv: 6040 interactions\n",
      "📊 Chargé clicks_hour_353.csv: 8063 interactions\n",
      "📊 Chargé clicks_hour_354.csv: 9749 interactions\n",
      "📊 Chargé clicks_hour_355.csv: 8532 interactions\n",
      "📊 Chargé clicks_hour_356.csv: 6981 interactions\n",
      "📊 Chargé clicks_hour_357.csv: 6767 interactions\n",
      "📊 Chargé clicks_hour_358.csv: 5310 interactions\n",
      "📊 Chargé clicks_hour_359.csv: 3821 interactions\n",
      "📊 Chargé clicks_hour_360.csv: 2636 interactions\n",
      "📊 Chargé clicks_hour_361.csv: 1468 interactions\n",
      "📊 Chargé clicks_hour_362.csv: 853 interactions\n",
      "📊 Chargé clicks_hour_363.csv: 764 interactions\n",
      "📊 Chargé clicks_hour_364.csv: 1245 interactions\n",
      "📊 Chargé clicks_hour_365.csv: 2919 interactions\n",
      "📊 Chargé clicks_hour_366.csv: 6976 interactions\n",
      "📊 Chargé clicks_hour_367.csv: 14112 interactions\n",
      "📊 Chargé clicks_hour_368.csv: 16506 interactions\n",
      "📊 Chargé clicks_hour_369.csv: 14301 interactions\n",
      "📊 Chargé clicks_hour_370.csv: 12445 interactions\n",
      "📊 Chargé clicks_hour_371.csv: 11506 interactions\n",
      "📊 Chargé clicks_hour_372.csv: 15050 interactions\n",
      "📊 Chargé clicks_hour_373.csv: 13641 interactions\n",
      "📊 Chargé clicks_hour_374.csv: 11083 interactions\n",
      "📊 Chargé clicks_hour_375.csv: 11492 interactions\n",
      "📊 Chargé clicks_hour_376.csv: 9491 interactions\n",
      "📊 Chargé clicks_hour_377.csv: 8553 interactions\n",
      "📊 Chargé clicks_hour_378.csv: 6910 interactions\n",
      "📊 Chargé clicks_hour_379.csv: 6392 interactions\n",
      "📊 Chargé clicks_hour_380.csv: 6588 interactions\n",
      "📊 Chargé clicks_hour_381.csv: 7189 interactions\n",
      "📊 Chargé clicks_hour_382.csv: 6051 interactions\n",
      "📊 Chargé clicks_hour_383.csv: 3682 interactions\n",
      "📊 Chargé clicks_hour_384.csv: 2569 interactions\n",
      "🎯 Total interactions chargées: 2988181\n",
      "\n",
      "📈 STATISTIQUES DU DATASET:\n",
      "  nb_users: 322,897\n",
      "  nb_articles: 46,033\n",
      "  nb_sessions: 1,048,594\n",
      "  nb_interactions: 2,988,181\n",
      "  articles_metadata_count: 364,047\n"
     ]
    }
   ],
   "source": [
    "def load_clicks_data(clicks_dir: Path, sample_files: int = 385) -> pd.DataFrame:\n",
    "    \"\"\"Charge les données de clics (échantillon pour développement).\"\"\"\n",
    "    click_files = sorted(list(clicks_dir.glob(\"clicks_hour_*.csv\")))\n",
    "    \n",
    "    if not click_files:\n",
    "        raise FileNotFoundError(\"❌ Aucun fichier de clics trouvé\")\n",
    "    \n",
    "    # Limiter pour développement\n",
    "    selected_files = click_files[:sample_files]\n",
    "    \n",
    "    dfs = []\n",
    "    for file_path in selected_files:\n",
    "        df_chunk = pd.read_csv(file_path)\n",
    "        dfs.append(df_chunk)\n",
    "        print(f\"📊 Chargé {file_path.name}: {df_chunk.shape[0]} interactions\")\n",
    "    \n",
    "    clicks_df = pd.concat(dfs, ignore_index=True)\n",
    "    print(f\"🎯 Total interactions chargées: {clicks_df.shape[0]}\")\n",
    "    return clicks_df\n",
    "\n",
    "def explore_basic_stats(clicks_df: pd.DataFrame, articles_df: pd.DataFrame) -> dict:\n",
    "    \"\"\"Calcule les statistiques de base du dataset.\"\"\"\n",
    "    stats = {\n",
    "        'nb_users': clicks_df['user_id'].nunique(),\n",
    "        'nb_articles': clicks_df['click_article_id'].nunique(),\n",
    "        'nb_sessions': clicks_df['session_id'].nunique(),\n",
    "        'nb_interactions': len(clicks_df),\n",
    "        'articles_metadata_count': len(articles_df)\n",
    "    }\n",
    "    \n",
    "    print(\"\\n📈 STATISTIQUES DU DATASET:\")\n",
    "    for key, value in stats.items():\n",
    "        print(f\"  {key}: {value:,}\")\n",
    "    \n",
    "    return stats\n",
    "\n",
    "# Chargement des données de clics\n",
    "clicks_df = load_clicks_data(data_paths['clicks_dir'])\n",
    "dataset_stats = explore_basic_stats(clicks_df, articles_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6dd86d",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "## 2. Content-Based Filtering Implementation\n",
    "\n",
    "Utilise les embeddings précalculés pour calculer la similarité entre articles.\n",
    "Stratégies implémentées:\n",
    "- Dernier article cliqué par l'utilisateur\n",
    "- Moyenne pondérée des articles cliqués\n",
    "- Gestion des nouveaux utilisateurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "401d148e",
   "metadata": {
    "title": "[python]"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "👥 Matrice créée: 322897 users, 46033 articles\n",
      "🎯 Content-Based pour user 0: [87278, 86143, 87676, 87030, 87427]\n"
     ]
    }
   ],
   "source": [
    "def create_user_article_matrix(clicks_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Crée une matrice utilisateur-article avec comptage des clics.\"\"\"\n",
    "    user_articles = clicks_df.groupby(['user_id', 'click_article_id']).size().reset_index()\n",
    "    user_articles.columns = ['user_id', 'article_id', 'click_count']\n",
    "    \n",
    "    print(f\"👥 Matrice créée: {user_articles['user_id'].nunique()} users, \" +\n",
    "          f\"{user_articles['article_id'].nunique()} articles\")\n",
    "    return user_articles\n",
    "\n",
    "def get_user_last_article(clicks_df: pd.DataFrame, user_id: int) -> int:\n",
    "    \"\"\"Retourne le dernier article cliqué par un utilisateur.\"\"\"\n",
    "    user_clicks = clicks_df[clicks_df['user_id'] == user_id]\n",
    "    \n",
    "    if user_clicks.empty:\n",
    "        return None\n",
    "        \n",
    "    last_click = user_clicks.loc[user_clicks['click_timestamp'].idxmax()]\n",
    "    return last_click['click_article_id']\n",
    "\n",
    "def compute_cosine_similarities(embeddings: np.ndarray, article_idx: int) -> np.ndarray:\n",
    "    \"\"\"Calcule la similarité cosinus entre un article et tous les autres.\"\"\"\n",
    "    if article_idx >= len(embeddings):\n",
    "        raise ValueError(f\"Index article {article_idx} hors limites\")\n",
    "    \n",
    "    article_embedding = embeddings[article_idx].reshape(1, -1)\n",
    "    similarities = cosine_similarity(article_embedding, embeddings)[0]\n",
    "    return similarities\n",
    "\n",
    "def content_based_recommendations(user_id: int, embeddings: np.ndarray, \n",
    "                                clicks_df: pd.DataFrame, n_recommendations: int = 5) -> list:\n",
    "    \"\"\"Génère des recommandations content-based pour un utilisateur.\"\"\"\n",
    "    # Stratégie: utiliser le dernier article cliqué\n",
    "    last_article_id = get_user_last_article(clicks_df, user_id)\n",
    "    \n",
    "    if last_article_id is None:\n",
    "        return []  # Utilisateur inconnu\n",
    "    \n",
    "    # Calculer similarités\n",
    "    similarities = compute_cosine_similarities(embeddings, last_article_id)\n",
    "    \n",
    "    # Obtenir les articles déjà vus par l'utilisateur\n",
    "    user_articles = set(clicks_df[clicks_df['user_id'] == user_id]['click_article_id'])\n",
    "    \n",
    "    # Trier par similarité décroissante, exclure articles déjà vus\n",
    "    article_scores = [(i, score) for i, score in enumerate(similarities) \n",
    "                     if i not in user_articles and score > 0]\n",
    "    article_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Retourner top N recommandations\n",
    "    recommendations = [article_id for article_id, _ in article_scores[:n_recommendations]]\n",
    "    return recommendations\n",
    "\n",
    "# Test du système content-based\n",
    "user_articles_matrix = create_user_article_matrix(clicks_df)\n",
    "test_user_id = clicks_df['user_id'].iloc[0]\n",
    "cb_recommendations = content_based_recommendations(test_user_id, embeddings_matrix, clicks_df)\n",
    "print(f\"🎯 Content-Based pour user {test_user_id}: {cb_recommendations}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf1452b",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "## 3. Collaborative Filtering avec Surprise Library\n",
    "\n",
    "Implémentation obligatoire selon les consignes du projet.\n",
    "Utilise des ratings implicites basés sur le nombre de clics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db80d3de",
   "metadata": {
    "title": "[python]"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⭐ Ratings créés: 2950710 interactions, rating moyen: 1.14\n",
      "🤖 Modèle SVD entraîné - RMSE: 0.031\n",
      "🤝 Collaborative pour user 0: [68851, 257732, 30315, 291167, 355845]\n"
     ]
    }
   ],
   "source": [
    "def create_implicit_ratings(clicks_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Crée des ratings implicites basés sur les clics.\"\"\"\n",
    "    # Compter les clics par utilisateur-article\n",
    "    ratings = clicks_df.groupby(['user_id', 'click_article_id']).agg({\n",
    "        'session_size': 'mean',  # Taille moyenne des sessions\n",
    "        'click_timestamp': 'count'  # Nombre de clics\n",
    "    }).reset_index()\n",
    "    \n",
    "    ratings.columns = ['user_id', 'article_id', 'avg_session_size', 'click_count']\n",
    "    \n",
    "    # Créer rating implicite : combinaison clics + session size\n",
    "    ratings['implicit_rating'] = (\n",
    "        ratings['click_count'] * 2 +  # Poids des clics répétés\n",
    "        ratings['avg_session_size'] * 0.1  # Poids de l'engagement session\n",
    "    )\n",
    "    \n",
    "    # Normaliser entre 1 et 5 pour Surprise\n",
    "    max_rating = ratings['implicit_rating'].max()\n",
    "    ratings['rating'] = 1 + (ratings['implicit_rating'] / max_rating) * 4\n",
    "    \n",
    "    print(f\"⭐ Ratings créés: {len(ratings)} interactions, \" +\n",
    "          f\"rating moyen: {ratings['rating'].mean():.2f}\")\n",
    "    \n",
    "    return ratings[['user_id', 'article_id', 'rating']]\n",
    "\n",
    "def train_surprise_model(ratings_df: pd.DataFrame, algorithm='SVD') -> tuple:\n",
    "    \"\"\"Entraîne un modèle Surprise sur les ratings implicites.\"\"\"\n",
    "    # Préparer les données pour Surprise\n",
    "    reader = Reader(rating_scale=(1, 5))\n",
    "    data = Dataset.load_from_df(ratings_df, reader)\n",
    "    \n",
    "    # Split train/test\n",
    "    trainset, testset = train_test_split(data, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Sélection de l'algorithme\n",
    "    if algorithm == 'SVD':\n",
    "        model = SVD(random_state=42, n_factors=50, n_epochs=20)\n",
    "    elif algorithm == 'NMF':\n",
    "        model = NMF(random_state=42, n_factors=50, n_epochs=20)\n",
    "    else:\n",
    "        raise ValueError(f\"Algorithme {algorithm} non supporté\")\n",
    "    \n",
    "    # Entraînement\n",
    "    model.fit(trainset)\n",
    "    \n",
    "    # Évaluation\n",
    "    predictions = model.test(testset)\n",
    "    rmse = accuracy.rmse(predictions, verbose=False)\n",
    "    \n",
    "    print(f\"🤖 Modèle {algorithm} entraîné - RMSE: {rmse:.3f}\")\n",
    "    return model, trainset\n",
    "\n",
    "def collaborative_recommendations(user_id: int, model, trainset, \n",
    "                                n_recommendations: int = 5) -> list:\n",
    "    \"\"\"Génère des recommandations collaborative filtering.\"\"\"\n",
    "    try:\n",
    "        # Vérifier si l'utilisateur existe dans le trainset\n",
    "        inner_user_id = trainset.to_inner_uid(user_id)\n",
    "    except ValueError:\n",
    "        # Utilisateur inconnu\n",
    "        return []\n",
    "    \n",
    "    # Obtenir tous les articles du trainset (inner IDs)\n",
    "    all_inner_items = set(range(trainset.n_items))\n",
    "    \n",
    "    # Articles déjà vus par l'utilisateur (inner IDs)\n",
    "    user_items = set([item for (item, _) in trainset.ur[inner_user_id]])\n",
    "    \n",
    "    # Articles non vus (inner IDs)\n",
    "    unseen_inner_items = all_inner_items - user_items\n",
    "    \n",
    "    # Prédire les ratings pour les articles non vus\n",
    "    predictions = []\n",
    "    for inner_item_id in unseen_inner_items:\n",
    "        # Convertir inner ID vers raw ID pour la prédiction\n",
    "        raw_item_id = trainset.to_raw_iid(inner_item_id)\n",
    "        pred = model.predict(user_id, raw_item_id)\n",
    "        predictions.append((raw_item_id, pred.est))\n",
    "    \n",
    "    # Trier par rating prédit décroissant\n",
    "    predictions.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Retourner top N recommandations (raw IDs)\n",
    "    recommendations = [article_id for article_id, _ in predictions[:n_recommendations]]\n",
    "    return recommendations\n",
    "\n",
    "# Entraînement du modèle collaboratif\n",
    "ratings_df = create_implicit_ratings(clicks_df)\n",
    "surprise_model, trainset = train_surprise_model(ratings_df, algorithm='SVD')\n",
    "\n",
    "# Test collaborative filtering\n",
    "cf_recommendations = collaborative_recommendations(test_user_id, surprise_model, trainset)\n",
    "print(f\"🤝 Collaborative pour user {test_user_id}: {cf_recommendations}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1611650",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "## 4. Système Hybride\n",
    "\n",
    "Combine les approches Content-Based et Collaborative Filtering.\n",
    "Différentes stratégies de combinaison selon les performances individuelles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24c8e1ad",
   "metadata": {
    "title": "[python]"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔗 Hybride pour user 0: [87278, 86143, 87676, 87030, 68851]\n",
      "📊 Métriques couverture: {'nb_recommendations': 5, 'coverage_rate': 0.000108617730758369, 'diversity_rate': 1.0}\n"
     ]
    }
   ],
   "source": [
    "def hybrid_recommendations(user_id: int, embeddings: np.ndarray, clicks_df: pd.DataFrame,\n",
    "                          surprise_model, trainset, n_recommendations: int = 5,\n",
    "                          weight_cb: float = 0.6, weight_cf: float = 0.4) -> list:\n",
    "    \"\"\"\n",
    "    Génère des recommandations hybrides combinant CB et CF.\n",
    "    \n",
    "    Args:\n",
    "        weight_cb: Poids du Content-Based (0.0 à 1.0)\n",
    "        weight_cf: Poids du Collaborative Filtering (0.0 à 1.0)\n",
    "    \"\"\"\n",
    "    # Obtenir recommandations des deux systèmes\n",
    "    cb_recs = content_based_recommendations(user_id, embeddings, clicks_df, n_recommendations*2)\n",
    "    cf_recs = collaborative_recommendations(user_id, surprise_model, trainset, n_recommendations*2)\n",
    "    \n",
    "    # Scoring hybride\n",
    "    hybrid_scores = {}\n",
    "    \n",
    "    # Scorer les recommandations Content-Based\n",
    "    for i, article_id in enumerate(cb_recs):\n",
    "        score = weight_cb * (1.0 - i / len(cb_recs))  # Score décroissant\n",
    "        hybrid_scores[article_id] = hybrid_scores.get(article_id, 0) + score\n",
    "    \n",
    "    # Scorer les recommandations Collaborative\n",
    "    for i, article_id in enumerate(cf_recs):\n",
    "        score = weight_cf * (1.0 - i / len(cf_recs))  # Score décroissant  \n",
    "        hybrid_scores[article_id] = hybrid_scores.get(article_id, 0) + score\n",
    "    \n",
    "    # Trier par score hybride décroissant\n",
    "    sorted_recommendations = sorted(hybrid_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Retourner top N\n",
    "    final_recommendations = [article_id for article_id, _ in sorted_recommendations[:n_recommendations]]\n",
    "    return final_recommendations\n",
    "\n",
    "def evaluate_recommendation_coverage(recommendations: list, total_articles: int) -> dict:\n",
    "    \"\"\"Évalue la couverture et diversité des recommandations.\"\"\"\n",
    "    metrics = {\n",
    "        'nb_recommendations': len(recommendations),\n",
    "        'coverage_rate': len(set(recommendations)) / total_articles if total_articles > 0 else 0,\n",
    "        'diversity_rate': len(set(recommendations)) / len(recommendations) if recommendations else 0\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "# Test du système hybride\n",
    "hybrid_recs = hybrid_recommendations(test_user_id, embeddings_matrix, clicks_df, \n",
    "                                   surprise_model, trainset)\n",
    "print(f\"🔗 Hybride pour user {test_user_id}: {hybrid_recs}\")\n",
    "\n",
    "# Évaluation\n",
    "total_articles = clicks_df['click_article_id'].nunique()\n",
    "coverage_metrics = evaluate_recommendation_coverage(hybrid_recs, total_articles)\n",
    "print(f\"📊 Métriques couverture: {coverage_metrics}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc92a2f4",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "## 5. Sauvegarde des Modèles et Configuration\n",
    "\n",
    "Préparation pour déploiement GCP Cloud Functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e5e704b8",
   "metadata": {
    "title": "[python]"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Modèles sauvegardés avec succès:\n",
      "  surprise_model: ../scripts/models/surprise_model.pkl\n",
      "  embeddings: ../scripts/models/embeddings.pkl\n",
      "  articles_metadata: ../scripts/models/articles_metadata.pkl\n",
      "  config: ../scripts/models/config.pkl\n"
     ]
    }
   ],
   "source": [
    "def save_models_for_deployment(surprise_model, embeddings: np.ndarray, \n",
    "                              articles_df: pd.DataFrame, output_dir: str = \"../scripts/models\") -> dict:\n",
    "    \"\"\"Sauvegarde les modèles entraînés pour déploiement.\"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    saved_files = {}\n",
    "    \n",
    "    # Sauvegarder le modèle Surprise\n",
    "    with open(f\"{output_dir}/surprise_model.pkl\", 'wb') as f:\n",
    "        pickle.dump(surprise_model, f)\n",
    "    saved_files['surprise_model'] = f\"{output_dir}/surprise_model.pkl\"\n",
    "    \n",
    "    # Sauvegarder les embeddings (possibilité PCA si trop volumineux)\n",
    "    if embeddings.shape[1] > 1000:  # Si > 1000 dimensions, appliquer PCA\n",
    "        pca = PCA(n_components=min(500, embeddings.shape[1]))\n",
    "        embeddings_reduced = pca.fit_transform(embeddings)\n",
    "        \n",
    "        with open(f\"{output_dir}/embeddings_pca.pkl\", 'wb') as f:\n",
    "            pickle.dump({'embeddings': embeddings_reduced, 'pca': pca}, f)\n",
    "        saved_files['embeddings'] = f\"{output_dir}/embeddings_pca.pkl\"\n",
    "        \n",
    "        print(f\"🗜️ Embeddings réduits: {embeddings.shape} → {embeddings_reduced.shape}\")\n",
    "    else:\n",
    "        with open(f\"{output_dir}/embeddings.pkl\", 'wb') as f:\n",
    "            pickle.dump(embeddings, f)\n",
    "        saved_files['embeddings'] = f\"{output_dir}/embeddings.pkl\"\n",
    "    \n",
    "    # Sauvegarder métadonnées articles essentielles\n",
    "    articles_meta = articles_df[['article_id', 'category_id', 'words_count']].copy()\n",
    "    articles_meta.to_pickle(f\"{output_dir}/articles_metadata.pkl\")\n",
    "    saved_files['articles_metadata'] = f\"{output_dir}/articles_metadata.pkl\"\n",
    "    \n",
    "    # Configuration système\n",
    "    config = {\n",
    "        'model_version': '1.0.0',\n",
    "        'n_recommendations': 5,\n",
    "        'hybrid_weights': {'content_based': 0.6, 'collaborative': 0.4},\n",
    "        'embeddings_shape': embeddings.shape,\n",
    "        'total_articles': len(articles_df),\n",
    "        'training_stats': dataset_stats\n",
    "    }\n",
    "    \n",
    "    with open(f\"{output_dir}/config.pkl\", 'wb') as f:\n",
    "        pickle.dump(config, f)\n",
    "    saved_files['config'] = f\"{output_dir}/config.pkl\"\n",
    "    \n",
    "    print(\"💾 Modèles sauvegardés avec succès:\")\n",
    "    for name, path in saved_files.items():\n",
    "        print(f\"  {name}: {path}\")\n",
    "    \n",
    "    return saved_files\n",
    "\n",
    "# Sauvegarde pour déploiement\n",
    "saved_model_paths = save_models_for_deployment(surprise_model, embeddings_matrix, articles_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b383df9d",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "## 6. Validation et Tests Finaux\n",
    "\n",
    "Tests de validation du système complet avant mise en production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a1eb877",
   "metadata": {
    "title": "[python]"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🧪 RÉSULTATS TESTS SYSTÈME:\n",
      "   user_id  content_based  collaborative  hybrid\n",
      "0        0              5              5       5\n",
      "1        1              5              5       5\n",
      "2        2              5              5       5\n",
      "3        3              5              5       5\n",
      "4        4              5              5       5\n",
      "\n",
      "📈 Statistiques finales:\n",
      "   - 2988181 interactions traitées\n",
      "   - 322897 utilisateurs\n",
      "   - 46033 articles\n",
      "   - Modèles prêts pour déploiement GCP\n"
     ]
    }
   ],
   "source": [
    "def test_recommendation_system(sample_users: list, n_tests: int = 5) -> pd.DataFrame:\n",
    "    \"\"\"Teste le système sur un échantillon d'utilisateurs.\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for user_id in sample_users[:n_tests]:\n",
    "        try:\n",
    "            # Test des 3 approches\n",
    "            cb_recs = content_based_recommendations(user_id, embeddings_matrix, clicks_df)\n",
    "            cf_recs = collaborative_recommendations(user_id, surprise_model, trainset)  \n",
    "            hybrid_recs = hybrid_recommendations(user_id, embeddings_matrix, clicks_df,\n",
    "                                               surprise_model, trainset)\n",
    "            \n",
    "            results.append({\n",
    "                'user_id': user_id,\n",
    "                'content_based': len(cb_recs),\n",
    "                'collaborative': len(cf_recs),\n",
    "                'hybrid': len(hybrid_recs),\n",
    "                'cb_recs': cb_recs[:3],  # 3 premières recommandations\n",
    "                'cf_recs': cf_recs[:3],\n",
    "                'hybrid_recs': hybrid_recs[:3]\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Erreur pour user {user_id}: {e}\")\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Tests de validation\n",
    "sample_user_ids = clicks_df['user_id'].unique()[:10].tolist()\n",
    "test_results = test_recommendation_system(sample_user_ids)\n",
    "\n",
    "print(\"\\n🧪 RÉSULTATS TESTS SYSTÈME:\")\n",
    "print(test_results[['user_id', 'content_based', 'collaborative', 'hybrid']])\n",
    "\n",
    "print(f\"\\n📈 Statistiques finales:\")\n",
    "print(f\"   - {len(clicks_df)} interactions traitées\")\n",
    "print(f\"   - {clicks_df['user_id'].nunique()} utilisateurs\")  \n",
    "print(f\"   - {clicks_df['click_article_id'].nunique()} articles\")\n",
    "print(f\"   - Modèles prêts pour déploiement GCP\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "title,-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": ".content_reco",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
